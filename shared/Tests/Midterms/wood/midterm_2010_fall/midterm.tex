% Essential Formatting
   
\documentclass[12pt]{article}
\usepackage{epsfig,amsmath,amsthm,amssymb}
\usepackage[questions]{urmathtest}[2001/05/12]
%\usepackage[answersheet]{urmathtest}[2001/05/12]
%\usepackage[answers]{urmathtest}[2001/05/12]

% For use with pdflatex
% \pdfpagewidth\paperwidth
% \pdfpageheight\paperheight

% Basic User Defs

\def\ds{\displaystyle}

\newcommand{\ansbox}[1]
{\work{
  \pos\hfill \framebox[#1][l]{ANSWER:\rule[-.3in]{0in}{.7in}}
}{}}

\newcommand{\ansrectangle}
{\work{
  \pos\hfill \framebox[6in][l]{ANSWER:\rule[-.3in]{0in}{.7in}}
}{}}

% Beginning of the Document

\begin{document}
\examtitle{LINEAR REGRESSION MODELS W4315}{Midterm Examination}{11/05/2009}
 \begin{center}
  Instructor: Frank Wood (10:35-11:50) 
 \end{center}
\studentinfo
\instructions{
  %\textbf{Circle your Instructor's Name along with the Lecture Time:}

 

  \begin{itemize}
  \item
    \textbf{Please show all your work.
            You may use back pages if necessary.}
  %\item
   % \textbf{Please put your \underline{simplified}
   %         final answers in the spaces provided.}
  \end{itemize}
}
\finishfirstpage

% Problems Start Here % ----------------------------------------------------- %


\problem{20} {
 The data below give weight $X$ (kg) and height $Y$ (cm)
 of 5 teenagers.

\begin{center}
\begin{tabular}{ll}
$X$ (kg) & $Y$ (cm) \\
\hline
50&160 \\
60&160 \\
70&170 \\
80&170 \\
90&180\\
\hline
\end{tabular}
\end{center}
\begin{enumerate}
\item (10 pts) Draw a scatter plot of height $Y$ versus weight $X$.
\item (10 pts) Fit a simple linear regression $ Y_i=\beta_0+\beta_1 X_i+\epsilon_i$  by finding the least squares estimators $b_0$ and $b_1$.
%\item (10 pts) In what sense are your estimators optimal? Please specify the minimum
%assumptions required for the optimality to hold.

\end{enumerate}

}
 { \vfill
  \answer
} {  }

\problem{25} {
Consider the same data set and the simple linear regression model specification
as given in the preceding problem. Suppose, in addition, $\epsilon_i$ are independent,
normally distributed with mean 0 and variance $\sigma^2$.
\begin{enumerate}
 % \item (5 pts) Find an unbiased estimator of $\sigma^2$.
 \item (5 pts) What's the sampling distribution of $b_1$?
  \item (5 pts) Find a 95\% confidence interval for $\beta_1$.
  \item (5 pts) Test one-sided hypothesis $H_0: \beta_1\leq 0$ versus $H_a: \beta_1>0$ (level of significance $\alpha=0.05$)
%  \item (5 pts) Find a 95\% Bonferroni Joint Confidence Interval of $\beta_0$ and $\beta_1$.
  \item (10 pts) Produce an ANOVA table and perform an F-test for $H_0: \beta_1=0$ versus $H_a: \beta_1\neq 0$. (level of significance: $\alpha=0.05$).
      \begin{table}[!h]
\begin{center}

\begin{tabular}{|c|c|c|c|}
\hline Source of Variation& SS& df& MS \\\hline
Regression&\qquad\qquad\qquad \qquad &\qquad\qquad\qquad\qquad&\qquad\qquad\qquad\qquad\\\hline
Error& &&\\\hline
Total& &&\\\hline
\end{tabular}
\end{center}
\end{table}
\end{enumerate}
}
 { \vfill
  \answer
} {  }
\newpage

\problem{30} {
Consider the classical regression setup
\[\bf{y}=\bf{X}\bf{\beta}+\bf{\epsilon}\]
where $\bf{\epsilon}\sim N(\bf{0},\sigma^2\bf{I})$. We know that the least square estimator for the parameter $\beta$ minimizes the residual sum of squares, which in matrix terms can be written $(\bf{y}-\bf{X}\beta)'(\bf{y}-\bf{X}\beta).$  The value of $\beta$ which minimizes this expression has the following analytic form
\begin{eqnarray}
\hat{\beta}=(\bf{X}'\bf{X})^{-1}\bf{X}'\bf{y}.
\end{eqnarray}
Suppose, though, that $\bf X'X$ is not invertible. In this case, this estimator can't be used.  To get around this problem we define a penalized residual sum of squares (this is called ``ridge regression'')
\begin{eqnarray}
(\bf{y}-\bf{X}\beta)'(\bf{y}-\bf{X}\beta)+\lambda \beta'\beta.
\label{ridge}
\end{eqnarray}

%Here, $\bf{X}$ doesn't have its first column as a $\bf{1}$ vector, so to speak, it only contains p covariates and it's an $n\times p$ matrix, which concomitantly means that $\beta$ is a $p\times1$ vector without considering the intercept. $\lambda$ is fixed scalar. 

\abcs
\item (20 pts) Derive the ridge regression estimate $\hat{\beta}^{ridge}$ in matrix form (show your work), this is equivalent to finding the $\beta$ which minimizes (2).
\item  (10 pts) In no more than 2 sentences, explain why the resulting estimator will work even when  $\bf X'X$ is singular.

\endabcs
%It should possess the form very much alike $\hat{\beta}$ in (\ref{ridge}) but taking into account the adjustment to the case that $\bf{X}'\bf{X}$ is non-invertible.
}
 { \vfill
  \answer
} {  }




\problem{40}
{
You, a statistical analysis consultant, are asked to analyze a regression problem. Food scientists invent a new secret ingredient to increase the fluffiness of pancakes (fluffiness is a made up concept but for our purposes will be defined as measurable scalar quantity). They know that the concentration of the new ingredient affects the fluffiness, and they want to figure out how large the effect is. We assume that a linear regression model is appropriate and the error comes from measurement of fluffiness. Two factories experimentally produce the pancakes with varying concentrations of the new ingredient. It could have been a simple linear regression problem, but the difficulty here is that the variances of fluffiness measurement of the pancakes produced by the two factories are DIFFERENT (that is why you are called). In other words, the fluffiness measurement errors were iid only for each factory individually. Denote fluffiness as $y=[y_1^1,y_1^2,\ldots,y_1^{n_1}, y_2^1,y_2^2,\ldots,y_2^{n_2}]$ and concentration level as $x=[x_1^1,x_1^2,\ldots,x_1^{n_1}, x_2^1,x_2^2,\ldots,x_2^{n_2}]$, where subscripts denote factories. 

  {\em Hint : here because the i.i.d. assumption is no longer valid, instead of only one parameter $\sigma^2$, we need to use two parameters $\sigma_1^2$ and $\sigma_2^2$.}  You may find it easier, after expressing the problem in matrix form, to simplify the likelihoods by expressing them in scalar form before seeking the ML estimates for some of the variables.



\abcs 
\item (15 pts) Assume (for now) that the variances $\sigma_1^2$ and $\sigma_2^2$ are {\em known}.   Using matrix notation derive a maximum likelihood estimator  for the vector of regression coefficients $\vec \beta = [\beta_0, \beta_1]^T$, i.e.~$\hat \beta = ?$.
\item (15 pts) Now assume that $\sigma^2_1$ and $\sigma^2_2$ are unknown.  Use maximum likelihood techniques to derive estimators for both (i.e. $\hat{\sigma_1^2} = ?$ and $\hat{\sigma_2^2} = ?$.  You can plug $\hat \beta$ in for $\beta$ in the final step.
\item (10 pts) Since the answer to (b) above forms a coupled system of equations, in two or fewer sentences describe a way to use the coupled equations to estimate all unknown parameters.
\endabcs
}
{
\vfill
\newpage
}
{
}






% Problems End Here % ------------------------------------------------------- %

\problemsdone
\end{document}

% End of the Document
