% Essential Formatting

\documentclass[12pt]{article}
\usepackage{epsfig,amsmath,amsthm,amssymb}
\usepackage[english]{babel}
%\usepackage{subfigure}
\usepackage[questions, answersheet]{urmathtest}[2001/05/12]
%\usepackage[answersheet]{urmathtest}[2001/05/12]
%\usepackage[answers]{urmathtest}[2001/05/12]


% For use with pdflatex
% \pdfpagewidth\paperwidth
% \pdfpageheight\paperheight

% Basic User Defs

\def\ds{\displaystyle}

\newcommand{\ansbox}[1]
{\work{
  \pos\hfill \framebox[#1][l]{ANSWER:\rule[-.3in]{0in}{.7in}}
}{}}

\newcommand{\ansrectangle}
{\work{
  \pos\hfill \framebox[6in][l]{ANSWER:\rule[-.3in]{0in}{.7in}}
}{}}


% Beginning of the Document

\begin{document}
\examtitle{LINEAR REGRESSION MODELS W4315}{HOMEWORK 4}{10/08/2009}
 \begin{center}
  Instructor: Frank Wood
 \end{center}
%%\studentinfo
\instructions{
  %\textbf{Circle your Instructor's Name along with the Lecture Time:}



  \begin{itemize}
  \item
    \textbf{Please show all your work.
            You may use back pages if necessary.}
  %\item
   % \textbf{Please put your \underline{simplified}
   %         final answers in the spaces provided.}
  \end{itemize}
}
\finishfirstpage

% Problems Start Here % ----------------------------------------------------- %


\problem{25} {
$A$ is an $n\times p$ matrix (in typical multiple regression settings, $n$ is the number of observations and $p$ is the number of parameters, and $n\ge p$), prove that
\begin{enumerate}
\item[(1)] $A'A$ and $AA'$ are symmetric matrices. ($A'$ denotes the transpose of $A$)
\item[(2)] $A'A$ and $AA'$ are semi-positive-definite matrices. (An $n\times n$ matrix $M$ is semi-positive-definite if $\forall x\in \mathbb{R}^n$, $x'Mx\ge0$.)
\item[(3)] If $A$ has full column rank ($rank(A)=p$), then prove $A'A$ is a positive-definite matrix. (An $n\times n$ matrix $M$ is positive-definite if $\forall$ nonzero $ x\in \mathbb{R}^n$, $x'Mx>0$.)
\end{enumerate}
}
 { \vfill
  \answer
} {
a. Verity the rule for product transpose $(AB)' = B'A'$.\\
b. $\forall x$, $x'A'Ax = (Ax)'Ax$. Notice that $Ax$ is a vector, denote it as $y = (y_1, y_2, \cdots, y_n)$, then
$$x'A'Ax = \sum^n_{i=1}y_i^2 \geq 0$$
c. We only need to show that when $A$ has full column rank, $x'A'Ax = 0$ implies $x = 0$. In
fact, $x'A'Ax = 0$ if and only if $Ax = 0$. But $A$ has full column rank, knowledge from linear
algebra tells us $Ax = 0$ implies $x = 0$.
}

\problem{25} {
 $A$ is an $n\times p$ matrix with full column rank. Let $P\equiv A(A'A)^{-1}A'$
\begin{enumerate}
\item[(1)] An $n\times n$ matrix $M$ is a projection matrix if it is symmetric and idempotent (i.e. $A^2=A$). Prove that $P$ is a projection matrix.
\item[(2)] Give the rank of $P$ and $I-P$. ($I$ is the $n\times n$ identity matrix)
\item[(3)] Prove that the projection $P$ is orthogonal. (i.e. $\forall x\in \mathbb{R}^n$, $(Px)'[(I-P)x]=0$)
\end{enumerate}
} { \vfill
  \answer
} {
a. It is trivial to verify that $P$ is symmetric and idempotent.\\
b. First, notice that projection matrices only have eigenvalues 0 and 1, which mean that its
rank equals to its trace. Then
$$rank(A(A'A)^{-1}A') = tr(A(A'A)^{-1}A') = tr((A'A)^{-1}A'A) = tr(I_p) = p$$
Similarly,
\begin{eqnarray*}
rank(I - A(A'A)^{-1}A') &=& tr(I_n - A(A'A)^{-1}A')\\
&=& tr(I_n) - tr(A(A'A)^{-1}A')\\
&=& n - tr((A'A)^{-1}A'A)\\
&=& n - tr(I_p) = n - p
\end{eqnarray*}
In the above displays, we use the two properties of trace, namely, $tr(A+B) = tr(A)+tr(B)$
and $tr(AB) = tr(BA)$.\\
c. Notice that $P'(I - P) = P - P^2 = 0$
 }

\problem{25} {
$\vec{X}$ is a 3 dimensional Gaussian random vector, with distribution $N_3(\mu, \Sigma)$, in which
\[\mu=\begin{pmatrix} 3\\4\\-3\end{pmatrix}, \Sigma=\begin{pmatrix} 2&1&3\\1&4&-2\\3&-2&8\end{pmatrix}\]
Let $Y_1=X_1+X_3$ and $Y_2=2X_2$, determine the distribution of $\vec{Y}=(Y_1,Y_2)'$ and the conditional distribution $Y_1|Y_2=10$.
} { \vfill
  \answer
} {
a. The distribution of $Y = (Y_1, Y_2)'$ is
$$Y \sim N(\begin{pmatrix} 0\\8\end{pmatrix}, \begin{pmatrix} 16&-2\\-2&16\end{pmatrix})$$
b. The conditional distribution is
$$Y_1|Y_2 = 10 \sim N(-.25, 15.75)$$
 }


\problem{10} {
Prove $\vec{Y}\sim N(0_{n\times1}, I_{n\times n})$ implies that all $Y_i$ i.i.d. follow $N(0,1)$. \\
(This problem may seem ridiculously easy to you. Just write out the joint density and see what it tells us about the marginal distributions.)
} { \vfill
  \answer
} {
This question is trivial. Just write out the joint density and see that it can be decomposed
into product of n standard normal densities.
 }


\problem{15} {\
Assume that an $n$ dimensional Gaussian random vector $X$ is distributed as $N(\mu,\Sigma)$
\begin{enumerate}
\item[(1)] Find a transformation of $X$, $Y=f(X)$, such that $Y\sim N(0_{n\times 1}, I_{n\times n})$.
\item[(2)] Prove that
\[(X-\mu)'\Sigma^{-1}(X-\mu)\sim \chi^2(n)\]
\end{enumerate}
} { \vfill
  \answer
} {
a. Just standardize X in matrix expressions.
$$Y = \Sigma^{-1/2}(X - \mu)$$
b. Because $Y \sim N(0_{n\times1}, I_{n\times n})$, so $Y'Y \sim \chi^2(n)$. Notice that
$$(X - \mu)'\Sigma^{-1}(X -\mu) = Y'Y $$}


% Problems End Here % ------------------------------------------------------- %

\problemsdone
\end{document}

% End of the Document
