% Essential Formatting

\documentclass[12pt]{article}
\usepackage{epsfig,amsmath,amsthm,amssymb}
\usepackage[questions, answersheet]{urmathtest}[2001/05/12]
%\usepackage[answersheet]{urmathtest}[2001/05/12]
%\usepackage[answers]{urmathtest}[2001/05/12]


% For use with pdflatex
% \pdfpagewidth\paperwidth
% \pdfpageheight\paperheight

% Basic User Defs

\def\ds{\displaystyle}

\newcommand{\ansbox}[1]
{\work{
  \pos\hfill \framebox[#1][l]{ANSWER:\rule[-.3in]{0in}{.7in}}
}{}}

\newcommand{\ansrectangle}
{\work{
  \pos\hfill \framebox[6in][l]{ANSWER:\rule[-.3in]{0in}{.7in}}
}{}}


% Beginning of the Document

\begin{document}
\examtitle{LINEAR REGRESSION MODELS W4315}{HOMEWORK 5}%{10/22/2009}

 \begin{center}
  Instructor: Frank Wood
 \end{center}
%%\studentinfo
\instructions{
  %\textbf{Circle your Instructor's Name along with the Lecture Time:}



  \begin{itemize}
  \item
    \textbf{Please show all your work.
            You may use back pages if necessary.}
  %\item
   % \textbf{Please put your \underline{simplified}
   %         final answers in the spaces provided.}
  \end{itemize}
} \finishfirstpage
% Problems Start Here % ----------------------------------------------------- %



\problem{20}{\footnote[1]{This is
problem 4.22 in `Applied Linear Regression Models(4th edition)' by
Kutner etc.}
Bonferroni inequality (4.2a) which is given as
\begin{center}
$P(\bar{A_1}\bigcap\bar{A_2})\ge 1-\alpha-\alpha=1-2\alpha$
\end{center}
deals with the case of two statements, $A_1$ and $A_2$. Extend the inequality to
the case of $n$ statements, namely, $A_1, A_2\ldots, A_n$, each with statement confidence coefficient $1-\alpha$.
}
{ \vfill
   \answer
}{
Following the thread on Page 155 in the textbook, we have:
\[P(A_1)=P(A_2)=\ldots=P(A_n)=\alpha\] then
\[P(\cap_{i=1}^n \bar A_i)=P(\overline {\cup_{i=1}^n A_i})=1-P(\cup_{i=1}^n A_i)\ge 1-\sum_{i=1}^n P(A_i)\]
So we have $P(\cap_{i=1}^n \bar A_i) \ge 1-n\alpha$
}


\problem{40}{\footnote[3]{This is
problem 6.27 in `Applied Linear Regression Models(4th edition)' by
Kutner etc.}
In a small-scale regression study, the following data were obtained:
\begin{table}[htdp]
\begin{center}
\begin{tabular}{rcrcrcrcrcrcrc}
\textbf{i:} &\textbf{1} &\textbf{2}& \textbf{3}&\textbf{4}&\textbf{5}&\textbf{6}\\
\hline \textbf{$X_{i1}$} &7 &4 &16 &3 &21 &8 \\
\textbf{$X_{i2}$} &33 & 41& 7 &49 & 5 & 31\\
\textbf{$Y_i$} &42 &33 &75 &28 &91 &55
\end{tabular}
\end{center}
\end{table}
Assume that regression model (1) which is:
\begin{eqnarray}
Y_i=\beta_0+\beta_1 X_{i1}+\beta X_{i2}+\epsilon_i
\end{eqnarray}
with independent normal error terms is appropriate. Using matrix methods, obtain (a) $\textbf{b}$; (b) $\textbf{e};$ (c) $\textbf{H}$; (d) SSR; (e) $s^2\{\textbf{b}\}$; (f) $\hat{Y_h}$ when $X_{h1}=10$, $X_{h2}=30$; (g) $s^2\{\hat{Y_h}\}$ when $X_{h1}=10$, $X_{h2}=30$. For the notations, please refer to section 6.4.
}
{\vfill
  \answer
}{
a. $$\begin{pmatrix} 33.93210\\2.78476\\-.26442\end{pmatrix}$$
b. $$\begin{pmatrix} -2.6996\\-1.2300\\-1.6374\\-1.3299\\-.0900\\6.98682\end{pmatrix}$$
c. $$\begin{pmatrix} .2314 &.2517& .2118& .1489& -.0548 &.2110\\
.2517& .3124& .0944& .2663& -.1479& .2231\\
.2118& .0944& .7044& -.3192& .1045& .2041\\
.1489& .2663& -.3192& .6143& .1414& .1483\\
-.0548& -.1479& .1045& .1414& .9404& .0163\\
.2110& .2231& .2041& .1483& .0163& .1971\end{pmatrix}$$
d. 3,009.926\\
e. $$\begin{pmatrix} 715.4711 &-34.1589& -13.5949\\
-34.1589& 1.6617& .6441\\
-13.5949& .6441& .2625 \end{pmatrix}$$
f. 53.8471\\
g. 5.4247


}
\problem{40} {
Consider the classical regression setup
\[\bf{y}=\bf{X}\bf{\beta}+\bf{\epsilon}\]
We want to find the maximum likelihood estimate of the parameters.\\
a. if $\bf{\epsilon}\sim N(\bf{0},\sigma^2\bf{I})$. Derive the maximum likelihood estimate of $\beta$ and $\sigma^2$.\\
b. if $\bf{\epsilon}\sim N(\bf{0},\Sigma)$ and $\Sigma$ is known. Derive the maximum likelihood estimate of $\beta$.
}
{ \vfill
  \answer
}{
  a. The log likelihood is proportional to
  \[-n\log(\sigma)-\frac{(y-X\beta)^T\Sigma^{-1}(y-X\beta)}{2\sigma^2}\]
  take derivatives with respect to $\beta$ and $\sigma$, we have
  \[\left\{
    \begin{array}{l}  -n/\sigma+\frac{(y-X\beta)^T\Sigma^{-1}(y-X\beta)}{\sigma^3}=0\\
      X^TX\beta-X^Ty=0
     \end{array} \right. \]
  The second equation is becasue
  \begin{align*}
    &\frac{\partial\beta^TA\beta}{\partial \beta}=2A\beta\\
    &\frac{\partial \alpha\beta}{\partial\beta}=\alpha^T
  \end{align*}
  So
  \begin{align*}
   &\hat{\beta}=(X^TX)^{-1}X^Ty\\
   &\hat{\sigma^2}=\frac{1}{n}y^T(I-X(X^TX)^{-1}X^T)y=\frac{\text{SSE}}{n}
  \end{align*}
b. Very similarly, we have when $\bf{\epsilon}\sim N(\bf{0},\Sigma)$, the MLE of $\beta$ is
\[\hat{\beta}=(X^T\Sigma^{-1}X)X^T\Sigma^{-1}y\]
}





\end{document}
