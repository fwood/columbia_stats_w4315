% Essential Formatting

\documentclass[12pt]{article}
\usepackage{epsfig,amsmath,amsthm,amssymb}
\usepackage[questions, answersheet]{urmathtest}[2001/05/12]
%\usepackage[answersheet]{urmathtest}[2001/05/12]
%\usepackage[answers]{urmathtest}[2001/05/12]


% For use with pdflatex
% \pdfpagewidth\paperwidth
% \pdfpageheight\paperheight

% Basic User Defs

\def\ds{\displaystyle}

\newcommand{\ansbox}[1]
{\work{
  \pos\hfill \framebox[#1][l]{ANSWER:\rule[-.3in]{0in}{.7in}}
}{}}

\newcommand{\ansrectangle}
{\work{
  \pos\hfill \framebox[6in][l]{ANSWER:\rule[-.3in]{0in}{.7in}}
}{}}


% Beginning of the Document

\begin{document}
\examtitle{LINEAR REGRESSION MODELS W4315}{HOMEWORK 5}%{10/22/2009}

 \begin{center}
 Due: 03/04/10\\
  Instructor: Frank Wood
 \end{center}
%%\studentinfo
\instructions{
  %\textbf{Circle your Instructor's Name along with the Lecture Time:}



  \begin{itemize}
  \item
    \textbf{Please show all your work.
            You may use back pages if necessary.}
  %\item
   % \textbf{Please put your \underline{simplified}
   %         final answers in the spaces provided.}
  \end{itemize}
} \finishfirstpage
% Problems Start Here % ----------------------------------------------------- %


\problem{20} { In order to get a maximum likelihood estimate of the
parameters of a Box-Cox transformed simple linear regression model
($Y_i^\lambda=\beta_0+\beta_1X_i+\epsilon_i$), we need to find the
gradient of the likelihood with respect to its parameters (the
gradient consists of the partial derivatives of the likelihood
function w.r.t.~all of the parameters). Derive the partial
derivatives of the likelihood w.r.t all parameters assuming that
$\epsilon_i\sim N(0,\sigma^2)$. (N.B.~the parameters here are
$\lambda,\beta_0,\beta_1,\sigma$)\\
(Extra Credit: Consider the transformation of the $Y_i$'s mentioned in the lecture notes but not covered in class.  Explain why this transformation is necessary in a gradient ascent procedure for $\lambda$)
 }
 { \vfill
  \answer
} { The gradient of a multi-variate function is defined to be a
vector consisting of all the partial derivatives w.r.t every single
variable. So we need to write down the full likelihood first:\\
$L=\prod\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{\sum(y_i^\lambda-\beta_0-\beta_1 x_i)^2}{2\sigma^2}}$\\
Then the log-likelihood function is: \\
$l=-\frac{n}{2}log\sigma^2-\frac{\sum(y_i^\lambda-\beta_0-\beta_1 x_i)^2}{2\sigma^2}$\\
Take derivatives w.r.t to all the four parameters, we have the followings:\\
\begin{eqnarray}
\frac{\partial l}{\partial \lambda}=-\frac{1}{\sigma^2}\sum(y_i^\lambda-\beta_0-\beta_1 x_i)y_i^\lambda ln y_i\\
\frac{\partial l}{\partial \beta_0}=\frac{1}{\sigma^2}\sum(y_i^\lambda-\beta_0-\beta_1 x_i)\\
\frac{\partial l}{\partial \beta_1}=\frac{1}{\sigma^2}\sum(y_i^\lambda-\beta_0-\beta_1 x_i)x_i\\
\frac{\partial l}{\partial \sigma^2}=-\frac{n}{2\sigma^2}+\frac{\sum(y_i^\lambda-\beta_0-\beta_1 x_i)^2}{2\sigma^4}
\end{eqnarray}
From the above equations array, we can have the gradient.\\
Extra credit: It's easily seen that equation (1) has no analytical form of the solution. So in order to find the $\lambda$ we are seeking for, we should try a sequence
of $\lambda$'s and figure out the MLE of all other parameters when $\lambda$ taking corresponding value. Then using the MLE of all the parameters we evaluate the corresponding
log-likelihood and find the largest one. Then $\lambda$ matching this max log-likelihood generates the appropriate Box-Cox transformation.}

\problem{15}{\footnote[1]{This is
problem 4.22 in `Applied Linear Regression Models(4th edition)' by
Kutner etc.} 
Derive an extension of Bonferroni inequality (4.2a) which is given as
\begin{center}
$P(\bar{A_1}\bigcap\bar{A_2})\ge 1-\alpha-\alpha=1-2\alpha$
\end{center}
for the case of three statements, each with statement confidence coefficient $1-\alpha$.
}
{ \vfill
   \answer
}{
Following the thread on Page 155 in the textbook, we have:\\
$Suppose P(A_1)=P(A_2)=P(A_3)=\alpha$, then\\
$P(\bar A_1 \cap \bar A_2 \cap \bar A_3)=P(\bar {A_1 \cup A_2 \cup A_3})=1-P(A_1 \cup A_2 \cup A_3)=1-{P(A_1)+P(A_2)+P(A_3)-P(A_1A_2)-P(A_1A_3)-P(A_2A_3)+P(A_1A_2A_3)}=1-3\alpha+P(A_1A_2)+P(A_1A_3)+P(A_2A_3)-P(A_1A_2A_3)$\\
So we have $P(\bar A_1 \cap \bar A_2 \cap \bar A_3) \ge 1-P(A_1)-P(A_2)-P(A_3)$
}

\problem{25}{\footnote[2]{This is
problem 5.24 in `Applied Linear Regression Models(4th edition)' by
Kutner etc.} 
Refer to \textbf{Consumer finance} Problems 5.5 and 5.13.\\
a. Using matrix methods, obtain the following: (1) vector of estimated regression coefficients, (2) vector of residuals, (3) SSR, (4) SSE, (5) estimated variance-covariance matrix of $\textbf{b}$, (6) point estimate of $E\{Y_h\}$ when $X_h=4$, (7) $s^2$\{pred\} when $X_h=4$\\
b. From your estimated variance-covariance matrix in part (a5), obtain the following: (1) $s\{b_0,b_1\}$; (2) $s^2\{b_0\}$; (3) $s\{b_1\}$\\
c. Find the hat matrix $\textbf{H}$\\
d. Find $s^2\{\textbf{e}\}$
}
{\vfill
  \answer
}{}

\problem{25}{\footnote[3]{This is
problem 6.27 in `Applied Linear Regression Models(4th edition)' by
Kutner etc.} 
In a small-scale regression study, the following data were obtained:
\begin{table}[htdp]
\begin{center}
\begin{tabular}{rcrcrcrcrcrcrc}
\textbf{i:} &\textbf{1} &\textbf{2}& \textbf{3}&\textbf{4}&\textbf{5}&\textbf{6}\\
\hline \textbf{$X_{i1}$} &7 &4 &16 &3 &21 &8 \\
\textbf{$X_{i2}$} &33 & 41& 7 &49 & 5 & 31\\
\textbf{$Y_i$} &42 &33 &75 &28 &91 &55
\end{tabular}
\end{center}
\end{table}
Assume that regression model (1) which is:
\begin{eqnarray}
Y_i=\beta_0+\beta_1 X_{i1}+\beta X_{i2}+\epsilon_i
\end{eqnarray}
with independent normal error terms is appropriate. Using matrix methods, obtain (a) $\textbf{b}$; (b) $\textbf{e};$ (c) $\textbf{H}$; (d) SSR; (e) $s^2\{\textbf{b}\}$; (f) $\hat{Y_h}$ when $X_{h1}=10$, $X_{h2}=30$; (g) $s^2\{\hat{Y_h}\}$ when $X_{h1}=10$, $X_{h2}=30$
}
{\vfill
  \answer
}{}

\problem{15}{
 Consider the classical matrix approach to multiple regression, i.e.
 \begin{center}
 $\bf y=X\mathbf{\beta}+\mathbf{\epsilon}$
 \end{center}
 where $\mathbf{X}$ is a $n\times p$ design matrix whose first column is all 1's, $\epsilon\sim N(\textbf{0},\textbf{I})$ and $\bf I$ is an identity matrix. Prove the following:\\
 a. The sum of squares error $SSE={\mathbf{e}}'{\mathbf{e}}$ can be written in a matrix form:
 \begin{eqnarray}
 SSE=\textbf{y}'(\textbf{I}-\textbf{X}(\textbf{X}'\textbf{X})^{-1}\textbf{X}')\textbf{y}
 \end{eqnarray}
 b. We call the RHS of (2) a quadratic form. Prove that the matrix $\textbf{A}=\mathbf{I}-\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}'$ is an idempotent matrix.\\
 c. Prove that the rank of $\textbf{A}$ defined in part (b) is $n-p$.\\
 N.B. $p$ columns in design matrix means there are $p-1$ predictors plus 1 intercept term. In your solutions please clearly notate the dimensions of all of the matrices. 
}
{\vfill
  \answer
}{}









\end{document}