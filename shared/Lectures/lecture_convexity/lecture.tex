\documentclass{beamer}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{setspace}
% \usepackage{beamerthemesplit} // Activate for custom appearance

\title{Convexity}
\author{Dr. Frank Wood}

\date{}

\DeclareMathOperator*{\Ave}{\mathbb{E}}
\DeclareMathOperator*{\Var}{Var}

\begin{document}

\frame{\titlepage}



\frame[t] {
 \frametitle{Today: Normal Error Regression Model}
$$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$$
\begin{enumerate}
\item $Y_i$ value of the response variable in the $i^{th}$ trial
\item $\beta_0$ and $\beta_1$ are parameters
\item $X_i$ is a known constant, the value of the predictor variable in the $i^{th}$ trial
\item $\epsilon_i \sim_{iid} N(0,\sigma^2)$
\item $i = 1,\ldots,n$
\end{enumerate}
}

\frame[t] {
 \frametitle{Inferences concerning $\beta_1$}
Tests concerning $\beta_1$ (the slope) are often of interest,
particularly
\begin{eqnarray*}
H_0 : \beta_1 &=& 0 \\
H_a: \beta 1 &\neq& 0
\end{eqnarray*}
the null hypothesis model$$Y_i = \beta_0 + (0) X_i + \epsilon_i$$
implies that there is no relationship between Y and X
 }

\frame[t] {
 \frametitle{Review : Hypothesis Testing}
\begin{enumerate}
\item Elements of a statistical test

\begin{enumerate}
\item Null hypothesis, $H_0$
\item Alternative hypothesis, $H_a$
\item Test statistic
\item Rejection region
\end{enumerate}
\end{enumerate}
}

\frame[t] {
 \frametitle{Review : Hypothesis Testing - Errors}
\begin{enumerate}
\item Errors

\begin{enumerate}
\item A type I error is made if $H_0$ is rejected when $H_0$ is true.  The probability of a type I error is denoted by $\alpha$.  The value of $\alpha$ is called the level of the test.
\item A type II error is made if $H_0$ is accepted when $H_a$ is true.  The
probability of a type II error is denoted by $\beta$.
\end{enumerate}
\end{enumerate}
}

\frame[t] {
 \frametitle{P-value}
The p-value, or attained significance level, is the smallest level
of significance $\alpha$ for which the observed data indicate that
the null hypothesis should be rejected. }

\frame[t] {%%%change pic%%%
 \frametitle{Null Hypothesis}
If $\beta_1 = 0$ then with $95\%$ confidence the $b_1$ would fall in
some range around zero
\begin{figure}
  \includegraphics[height=60mm]{b1.png}
\end{figure}
}

\frame[t] {%%%change pic%%%
 \frametitle{Alternative Hypothesis : Least Squares Fit}
\begin{figure}
  \includegraphics[height=60mm]{alternative.png}
\end{figure}
}

\frame[t] {
 \frametitle{Testing This Hypothesis}
\begin{enumerate}
\item Only have a finite sample
\item Different finite set of samples (from the same population / source) will (almost always) produce different estimates of $\beta_0$ and $\beta_0$ $(b_0, b_1)$ given the same estimation procedure
\item $b_0$ and $b_1$ are random variables whose sampling distributions can be statistically characterized
\item Hypothesis tests can be constructed using these distributions.
\end{enumerate}
}

\frame[t] {
 \frametitle{Example : Sampling Dist. Of $b_1$}
\begin{enumerate}
\item The point estimator for $b_1$ is
\begin{eqnarray*}
b_1 &=& \frac{\sum(X_i - \bar X)(Y_i - \bar Y)}{\sum(X_i-\bar X)^2} \\
%b_0 &=& \bar Y - b_1 \bar X\\
%\bar X &=& \frac{\sum X_i}{n} \\
%\bar Y&=&\frac{\sum Y_i}{n}
\end{eqnarray*}
\item The sampling distribution for $b_1$ is the distribution over $b_1$ that occurs when the predictor variables $X_i$ are held fixed and the observed outputs are repeatedly sampled
\end{enumerate}
}

\frame[t] {
 \frametitle{Sampling Dist. Of $b_1$ In Normal Regr. Model }
\begin{enumerate}
\item For a normal error regression model the sampling distribution of $b_1$ is normal, with mean and variance given by
\begin{eqnarray*}
\Ave(b_1) &=& \beta_1\\
\Var(b_1) &=& \frac{\sigma^2}{\sum(X_i-\bar X)^2}
\end{eqnarray*}

\item To show this we need to go through a number of algebraic steps.

\end{enumerate}
}

\frame[t] {
 \frametitle{First step}
To show
$$\sum(X_i - \bar X)(Y_i - \bar Y) = \sum(X_i - \bar X)Y_i$$
we observe
\begin{eqnarray*}
\sum(X_i - \bar X)(Y_i - \bar Y) &=& \sum(X_i - \bar X)Y_i - \sum(X_i - \bar X)\bar Y \\
&=& \sum(X_i - \bar X)Y_i - \bar Y \sum(X_i - \bar X) \\
&=& \sum(X_i - \bar X)Y_i - \bar Y \sum(X_i) + \bar Y n \frac{\sum X_i}{n} \\
&=& \sum(X_i - \bar X)Y_i
\end{eqnarray*}
}

\frame[t] {
 \frametitle{Slope as linear combination of outputs}
$b_1$ can be expressed as a linear combination of the $Y_i's$
\begin{eqnarray*}
b_1 &=& \frac{\sum(X_i - \bar X)(Y_i - \bar Y)}{\sum(X_i-\bar X)^2} \\
&=& \frac{\sum(X_i - \bar X)Y_i}{\sum(X_i-\bar X)^2} \\
&=& \sum k_i Y_i
\end{eqnarray*}
where $$k_i = \frac{\sum(X_i - \bar X)}{\sum(X_i-\bar X)^2}$$ }

\frame[t] {
 \frametitle{Properties of the $k_i's$}
It can be shown that
\begin{eqnarray*}
\sum k_i &=& 0 \\
\sum k_i X_i &=& 1 \\
\sum k_i^2 &=& \frac{1}{\sum(X_i - \bar X)^2}
\end{eqnarray*}
(possible homework).  We will use these properties to prove various
properties of the sampling distributions of $b_1$ and $b_0$.

}

\frame[t] {
 \frametitle{Normality of $b_1's$ Sampling Distribution}
\begin{enumerate}
\item Useful fact:
\begin{enumerate}
\item A linear combination of independent normal random variables is normally distributed
\item More formally: when $Y_1,\ldots,Y_n$ are independent normal random variables, the linear combination
$a_1Y_1 + a_2Y_2 + \ldots + a_nY_n$ is normally distributed, with
mean $\sum a_i\Ave(Y_i)$ and variance $\sum a_i^2\Var(Y_i)$
\end{enumerate}
\end{enumerate}
}

\frame[t] {
 \frametitle{Normality of $b_1's$ Sampling Distribution}
Since $b_1$ is a linear combination of the $Y_i's$ and each $Y_i$ is
an independent normal random variable, then $b_1$ is distributed
normally as well
\begin{eqnarray*}
b_1 &=& \sum k_i Y_i, \, k_i = \frac{(X_i - \bar X)}{\sum(X_i-\bar
X)^2}
\end{eqnarray*}
}

\frame[t] {
 \frametitle{$b_1$ is an unbiased estimator}
This can be seen using two of the properties
\begin{eqnarray*}
E(b_1) &=& E(\sum k_i Y_i) = \sum k_i E(Y_i) = \sum k_i (\beta_0+\beta_1 X_i) \\
&=& \beta_0 \sum k_i + \beta_1 \sum k_i X_i \\
&=& \beta_0 (0) +  \beta_1  (1) \\
&=& \beta_1
\end{eqnarray*}
 }

\frame[t] {
 \frametitle{Variance of $b_1$}
Since the $Y_i$ are independent random variables with variance
$\sigma^2$ and the $k_i's$ are constants we get
\begin{eqnarray*}
V(b_1) &=& V(\sum k_i Y_i) = \sum k_i^2 V(Y_i)  \\
&=& \sum k_i^2 \sigma^2 = \sigma^2   \sum k_i^2 \\
&=& \sigma^2  \frac{1}{\sum(X_i - \bar X)^2}
\end{eqnarray*}
note that this assumes that we know $\sigma^2$. Can we?

}

\frame[t] {
 \frametitle{Estimated variance of $b_1$}
\begin{enumerate}
\item When we don't know $\sigma^2$ then we have to replace it with the MSE estimate
\item Remember
$$s^2 = MSE = \frac{SSE}{n-2} = \frac{\sum(Y_i-\hat Y_i)^2}{n-2} = \frac{\sum e_i^2} {n-2}$$
plugging in we get
\begin{eqnarray*}
V(b_1) &=&  \frac{\sigma^2}{\sum(X_i - \bar X)^2} \\
\hat V(b_1) &=&  \frac{s^2}{\sum(X_i - \bar X)^2}
\end{eqnarray*}
\end{enumerate}
}

\frame[t] {
 \frametitle{Digression : Gauss-Markov Theorem}
In a regression model where E(2i) = 0 and variance V(2i) = ?2 < 1
and 2i and 2j are uncorrelated for all i and j the least squares
estimators b0 and b1 and unbiased and have minimum variance among
all unbiased linear estimators.\\ Remember
\begin{eqnarray*}
b_1 &=& \frac{\sum(X_i - \bar X)(Y_i - \bar Y)}{\sum(X_i-\bar X)^2} \\
b_0 &=& \bar Y - b_1 \bar X
%\bar X &=& \frac{\sum X_i}{n} \\
%\bar Y&=&\frac{\sum Y_i}{n}
\end{eqnarray*}

}

\frame[t] {
 \frametitle{Proof}
\begin{enumerate}
\item The theorem states that $b_1$ as minimum variance among all unbiased linear estimators of the form
$$\hat \beta_1 = \sum c_i Y_i$$
\item As this estimator must be unbiased we have
\begin{eqnarray*}
E(\hat \beta_1) &=& \sum c_i E(Y_i) = \beta_1 \\
 &=& \sum c_i  (\beta_0 + \beta_1 X_i) = \beta_0 \sum c_i  + \beta_1 \sum c_i X_i  = \beta_1
\end{eqnarray*}
\end{enumerate}
}

\frame[t] {
 \frametitle{Proof cont.}
\begin{enumerate}
\item Given these constraints
\begin{eqnarray*}
\beta_0 \sum c_i  + \beta_1 \sum c_i X_i  = \beta_1
\end{eqnarray*}
clearly it must be the case that $\sum c_i =0$ and $\sum c_iX_i = 1$

\item The variance of this estimator is
\begin{eqnarray*}
V(\hat \beta_1) &=& \sum c_i^2 V(Y_i) = \sigma^2 \sum c_i^2
\end{eqnarray*}

\end{enumerate}
}

\frame[t] {
 \frametitle{Proof cont.}
Now define $c_i = k_i + d_i$ where the $k_i$ are the constants we
already defined and the $d_i$ are arbitrary constants.  Let's look
at the variance of the estimator
\begin{eqnarray*}
V(\hat \beta_1) &=& \sum c_i^2 V(Y_i) = \sigma^2 \sum (k_i+d_i)^2 \\
&=& \sigma^2(\sum k_i^2 + \sum d_i^2 + 2 \sum k_i d_i)
\end{eqnarray*}
Note we just demonstrated that $$\sigma^2 \sum k_i^2 = V(b_1)$$}

\frame[t] {
 \frametitle{Proof cont.}
Now by showing that $\sum k_id_i = 0$ we're almost done
\begin{eqnarray*}
 \sum k_i d_i &=& \sum k_i (c_i - k_i) \\
&=& \sum k_i (c_i - k_i) \\
&=& \sum k_i c_i - \sum k_i^2 \\
&=& \sum c_i \left(\frac{X_i - \bar X}{\sum(X_i - \bar X)^2}\right) - \frac{1}{\sum(X_i - \bar X)^2} \\
&=&  \frac{\sum c_i X_i - \bar X\sum c_i}{\sum(X_i - \bar X)^2} -
\frac{1}{\sum(X_i - \bar X)^2} = 0
\end{eqnarray*}
}

\frame[t] {
 \frametitle{Proof end}
So we are left with \begin{eqnarray*}
V(\hat \beta_1) &=& \sigma^2(\sum k_i^2 + \sum d_i^2 ) \\
&=& V(b_1) + \sigma^2(\sum d_i^2 )
\end{eqnarray*}
which is minimized when the $d_i's = 0$. This means that the least
squares estimator b1 has minimum variance among all unbiased linear
estimators.
 }

\frame[t] {
 \frametitle{Sampling Distribution of $(b_1 - \beta_1)/S(b_1)$}
\begin{enumerate}
\item $b_1$ is normally distributed so $(b_1-\beta_1)/(\Var(b_1)^{1/2})$ is a standard normal variable
\item We don't know $\Var(b_1)$ so it must be estimated from data.  We have already denoted it's estimate
\item Using this estimate we it can be shown that
$$\frac{b_1-\beta_1}{\hat{S}(b_1)} \sim t(n-2)$$
$$\hat S(b_1) = \sqrt{ \hat V(b_1)}$$
\end{enumerate}
}

\frame[t] {
 \frametitle{Where does this come from?}
\begin{enumerate}
\item We need to rely upon the following theorem\\
For the normal error regression model
$$\frac{SSE}{\sigma^2} = \frac{\sum (Y_i - \hat Y_i)^2}{\sigma^2} \sim \chi^2(n-2)$$
and is independent of b0 and b1
\item Intuitively this follows the standard result for the sum of squared normal random
variables\\
Here there are two linear constraints imposed by the regression
parameter estimation that each reduce the number of degrees of
freedom by one.

\end{enumerate}
}

\frame[t] {
 \frametitle{Another useful fact : t distribution}
Let z and $\chi^2(\nu)$ be independent random variables (standard
normal and $\chi^2$ respectively).  We then define a t random
variable as follows:
$$t(\nu) = \frac{z}{\sqrt{\frac{\chi^2(\nu)}{\nu}}}$$
This version of the t distribution has one parameter, the degrees of
freedom $\nu$

}

\frame[t] {
 \frametitle{Distribution of the studentized statistic}
To derive the distribution of this statistic, first we do the
following rewrite
$$\frac{b_1 - \beta_1}{{\hat S(b_1)}} = \frac{\frac{b_1 - \beta_1}{{ S(b_1)}}}{\frac{\hat S(b_1)}{ S(b_1)}}$$
$$\frac{\hat S(b_1)}{ S(b_1)} = \sqrt{\frac{\hat V(b_1)}{ V(b_1)}}$$}

\frame[t] {
 \frametitle{Studentized statistic cont.}
And note the following
$$\frac{\hat V(b_1)}{ V(b_1)} = \frac{\frac{MSE}{\sum(X_i-\bar X)^2}}{\frac{\sigma^2}{\sum(X_i-\bar X)^2}} = \frac{MSE}{\sigma^2} = \frac{SSE}{\sigma^2(n-2)}$$
where we know (by the given theorem) the distribution of the last
term is $\chi^2$ and indep. of $b_1$ and $b_0$
$$\frac{SSE}{\sigma^2(n-2)} \sim \frac{\chi^2(n-2)}{n-2}$$}

\frame[t] {
 \frametitle{Studentized statistic final}
But by the given definition of the t distribution we have our result
$$\frac{b_1-\beta_1}{\hat{S}(b_1)} \sim t(n-2)$$
because putting everything together we can see that $$\frac{b_1 -
\beta_1}{\hat S(b_1)} \sim
\frac{z}{\sqrt{\frac{\chi^2(n-2)}{n-2}}}$$ }

\frame[t] {
 \frametitle{Confidence Intervals and Hypothesis Tests}
Now that we know the sampling distribution of $b_1$ (t with n-2
degrees of freedom) we can construct confidence intervals and
hypothesis tests easily }


\end{document}
