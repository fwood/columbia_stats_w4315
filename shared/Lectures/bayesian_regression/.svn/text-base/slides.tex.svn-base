\documentclass{beamer}

% \usepackage{beamerthemesplit} // Activate for custom appearance

\title{A Bayesian Treatment of Linear Gaussian Regression}
\author{Frank Wood}
\date{\today}

\newcommand{\comment}[1]{}
\newcommand{\ponedec}{\mathcal{P}^\downarrow_1}
\newcommand{\pone}{\mathcal{P}_1}
\newcommand{\rank}[1]{\mathrm{RANK}\left[#1\right]}
\newcommand{\E}[1]{\mathrm{E}\left[#1\right]}
\newcommand{\py}{\mathcal{PY}}
\newcommand{\iid}{iid.}
\newcommand{\drawiid}{\stackrel{\text{iid}}{\sim}}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\indicator}[1]{\text{I}\left[ #1 \right]}
\newcommand{\pdcoag}{PD(d_1,0)-\text{COAG}}
\newcommand{\todo}{\textbf{*TODO*}}
\newcommand{\igram}{\text{$\infty$-gram}}
\newcommand{\Prob}{\text{P}}

\def\mm{sequence memoizer }
\def\MM{SM }

\def\pibf{{\boldsymbol{\pi}}}
\def\kapbf{\boldsymbol{\kappa}}
\def\Sigmabf{\boldsymbol{\Sigma}}
\def\mubf{\boldsymbol{\mu}}
\def\taubf{\boldsymbol{\tau}}
\def\thebf{\boldsymbol{\theta}}
\def\betabf{\boldsymbol{\beta}}
\def\rhobf{\boldsymbol{\rho}}
\def\phibf{\boldsymbol{\phi}}
\def\pbf{\mathbf{p}}
\def\qbf{\mathbf{q}}
\def\sbf{\mathbf{s}}
\def\tbf{\mathbf{t}}
\def\ybf{\mathbf{y}}
\def\wbf{\mathbf{w}}
\def\xbf{\mathbf{x}}
\def\rbf{\mathbf{r}}
\def\tbf{\mathbf{t}}
\def\kbf{\mathbf{k}}
\def\Xbf{\mathbf{X}}
\def\Vbf{\mathbf{V}}

\def\Xnew{\mathbf{X}_{new}}
\def\ynew{\mathbf{y}_{new}}

\def\eye{\mathbf{I}}

\def\0bf{\mathbf{0}}
\def\Ibf{\mathbf{I}}
\def\phibf{\mathbf{\phi}}
\def\Phibf{\mathbf{\Phi}}
\def\disteq{{\stackrel{D}{=}}}
\def\EE{{\mathbb{E}}}

\def\phiv{\varphi}
\def\phivbf{\boldsymbol{\varphi}}

\def\Ocal{\mathcal{O}}

\DeclareMathOperator*{\Bet}{Beta}
\DeclareMathOperator{\coag}{COAG}
\DeclareMathOperator{\frag}{FRAG}
\DeclareMathOperator*{\rnk}{RANK}
\DeclareMathOperator*{\gem}{GEM}
\DeclareMathOperator*{\pd}{PD}
\DeclareMathOperator*{\gd}{GDir}
\DeclareMathOperator*{\Dir}{Dir}
\DeclareMathOperator*{\N}{N}


\begin{document}

\frame{\titlepage}

%\section[Outline]{}
%\frame{\tableofcontents}



\section{Classical Regression Model}

%\subsection{Single Parameter Model}
\frame[t] {%
 \frametitle{Bayesian Approach to Classical Linear Regression}
 In classical linear regression we have the following model
 \[\ybf|\betabf, \sigma^2, \Xbf \sim N(\Xbf\betabf,\sigma^2 \eye)\]
 Unfortunately we often don't know the observation error $\sigma^2$ and, as well, we don't know the vector of linear weights $\betabf$ that relates the input(s) to the output.  \newline
 
 In Bayesian regression, we are interested in several inference objectives.  One is the posterior distribution of the model parameters, in particular the posterior distribution of the observation error variance given the inputs and the outputs.
 
 \[P(\sigma^2 | \Xbf, \ybf)\]
 }

%\subsection{Single Parameter Model}
\frame[t] {%
 \frametitle{Posterior Distribution of the Error Variance}
 Of course in order to derive  
 \[P(\sigma^2 | \Xbf, \ybf)\]
 
 We have to treat $\betabf$ as a nuisance parameter and integrate it out
 
 \begin{eqnarray*}
 P(\sigma^2 | \Xbf, \ybf) &=& \int P(\sigma^2, \betabf | \Xbf, \ybf) d\betabf \\
 &=& \int P(\sigma^2 | \betabf, \Xbf, \ybf) P(\betabf |  \Xbf, \ybf)  d\betabf
 \end{eqnarray*}
 }

%\subsection{Single Parameter Model}
\frame[t] {%
 \frametitle{Predicting a New Output for a (set of) new Input(s)}
Of particular interest is the ability to predict the distribution of output values for a new input
 \[P(\ynew | \Xbf, \ybf, \Xnew)\]
 
 Here we have to treat both $\sigma^2$ and $\betabf$ as a nuisance parameters and integrate them out
 
 \begin{eqnarray*}
\lefteqn{P(\ynew | \Xbf, \ybf, \Xnew)}\\
 &=& \int \int P(\ynew | \betabf, \sigma^2)  P( \sigma^2 | \betabf, \Xbf, \ybf) P(\betabf |  \Xbf, \ybf)  d\betabf, d\sigma^2
 \end{eqnarray*}
 }

%\subsection{Single Parameter Model}
\frame[t] {%
 \frametitle{Noninformative Prior for Classical Regression}
For both objectives, we need to place a prior on the model parameters  $\sigma^2$ and $\betabf$. 
We will choose a noninformative prior to demonstrate the connection between the Bayesian approach to multiple regression and the classical approach. 

 \[P(\sigma^2, \betabf) \propto \sigma^{-2}\]
 
 Is this a proper prior?  What form will the posterior take in this case?  Will it be proper? \newline
 
Clearly other priors can be imposed, priors that are more informative. 
 }

%\subsection{Single Parameter Model}
\frame[t] {%
 \frametitle{Posterior distribution of $\betabf$ given $\sigma^2$}
 Sometimes it is the case that $\betabf$ is known.  In such cases the posterior distribution over the model parameters collapses to the posterior over $\betabf$ alone.  Even when $\sigma^2$ is also unknown, the factorization of the posterior distribution
 
 \[P(\sigma^2,\betabf | \Xbf, \ybf)= P( \betabf| \sigma^2, \Xbf, \ybf) P(\sigma^2 |  \Xbf, \ybf) \]
 
 Suggests that determining the posterior distribution $ P( \betabf| \sigma^2, \Xbf, \ybf) $ will be of use as a step in posterior analyses.  
 
  }
  
  
%\subsection{Single Parameter Model}
\frame[t] {%
 \frametitle{Posterior distribution of $\betabf$ given $\sigma^2$}
  
Given our choice of (improper) prior we have
  
 \[P( \betabf| \sigma^2, \Xbf, \ybf) P(\sigma^2 |  \Xbf, \ybf) \propto \N(\ybf | \Xbf\betabf, \sigma^2\eye)\sigma^{-2}  \]

Which, plugging in the normal likelihood and ignoring terms that are not a function of $\betabf$ we have

\begin{eqnarray*}
P( \betabf| \sigma^2, \Xbf, \ybf) &\propto& P( \betabf| \sigma^2, \Xbf, \ybf) P(\sigma^2 |  \Xbf, \ybf)\\
&\propto& \exp(-\frac{1}{2} (\ybf - \Xbf\betabf)^T \frac{1}{\sigma^2} \eye (\ybf - \Xbf\betabf)) )
\end{eqnarray*}

 when we expand out the exponent we get an expression that looks like (again dropping terms that do not involve $\betabf$)
 
 \[ \exp(-\frac{1}{2} ( -2 \ybf^T \frac{1}{\sigma^2}\eye\Xbf\betabf +\betabf^T\Xbf^T \frac{1}{\sigma^2}\eye\Xbf\betabf))\]
 
  }

  %\subsection{Single Parameter Model}
\frame[t] {%
 \frametitle{Multivariate Quadratic Square Completion}
  
We recognize the familiar form of the exponent of a multivariate Gaussian in this expression and can derive the mean and the variance of the distribution of $\betabf | \sigma^2, \ldots$ by noting that

\[ (\betabf - \mubf_{\betabf})^T \Sigmabf^{-1}_{\betabf} (\betabf - \mubf_{\betabf}) = \betabf^T\Sigmabf^{-1}\betabf - 2\mubf_{\betabf}^T\Sigmabf^{-1}_{\betabf}\betabf + const\]

From this and the result from the previous slide
 
 \[ \exp(\frac{1}{2} -2 \ybf^T \frac{1}{\sigma^2}\eye\Xbf\betabf +\betabf^T\Xbf^T \frac{1}{\sigma^2}\eye\Xbf\betabf)\]

We can immediately identify $\Sigmabf^{-1}_{\betabf} = \Xbf^T \frac{1}{\sigma^2}\eye\Xbf$ and thus that $\Sigmabf_{\betabf} = \sigma^2(\Xbf^T\Xbf)^{-1}.$  Similarly we can solve for $\mubf_{\betabf}$ and we find 
\[ \mubf_{\betabf} = (\Xbf^T\Xbf)^{-1}\Xbf^T\ybf\]
  }
  
  \frame[t] {%
 \frametitle{Distribution of $\betabf$ given $\sigma^2$}
  Mirroring the classical approach to matrix regression we have that the distribution of the regression coefficients given the observation noise variance is
  \[ \betabf | \ybf, \Xbf, \sigma^2 \sim \N(\mubf_{\betabf},\Sigmabf_{\betabf})\]
where  $\Sigmabf_{\betabf} = \sigma^2(\Xbf^T\Xbf)^{-1}$and $ \mubf_{\betabf} = (\Xbf^T\Xbf)^{-1}\Xbf^T\ybf$ \newline


Note that $ \mubf_{\betabf} $ is the same as the maximum likelihood or least squares estimate $\hat\betabf = (\Xbf^T\Xbf)^{-1}\Xbf^T\ybf$ of the regression coefficients. \newline



Of course we don't usually know the observation noise variance $\sigma^2$ and have to simultaneously estimate it from the data.  To determine the distribution of this quantity we need a few facts.


  }
  
    \frame[t] {%
 \frametitle{Scaled inverse-chi-square distribution}
If $\theta \sim \mathrm{Inv-}\chi^2(\nu, s^2)$ then the pdf for $\theta$ is given by 

\begin{eqnarray*}
P(\theta) &=& \frac{(\nu/2)^{\nu/2}}{\Gamma(\nu/2)} \theta^{-(\nu/2 +1)} e^{(-\nu s^2/(2\theta))} \\
&\propto&  \theta^{-(\nu/2 +1)} e^{(-\nu s^2/(2\theta))}
\end{eqnarray*}

You can think of the scaled inverse chi squared distribution as the chi squared distribution where the sum of squares is explicit in the parameterization.  $\nu>0$ is the number of ``degrees of freedom'', $s > 0$ is the scale parameter.
  }
  
   \frame[t] {%
 \frametitle{Distribution of $\sigma^2$ given observations $\ybf$ and $\Xbf$}
  The posterior distribution of the observation noise can be derived by noting that
  
  \begin{eqnarray*}
   P(\sigma^2|\ybf, \Xbf) &=& \frac{P(\betabf,\sigma^2|\ybf,\Xbf)}{P(\betabf|\sigma^2, \ybf, \Xbf)} \\
   &\propto& \frac{P(\ybf | \betabf, \sigma^2, \Xbf)P(\betabf,\sigma^2|\Xbf)}{P(\betabf|\sigma^2, \ybf, \Xbf)}
  \end{eqnarray*}
  
  But we have all of these terms.  $P(\ybf | \betabf, \sigma^2, \Xbf)$ is the standard regression likelihood.  We have just solved for the posterior distribution of $\betabf$ given $\sigma^2$ and the rest, $P(\betabf|\sigma^2, \ybf, \Xbf)$ and we specified our prior $P(\sigma^2, \betabf) \propto \sigma^{-2}$
  
    }

   \frame[t] {%
 \frametitle{Distribution of $\sigma^2$ given observations $\ybf$ and $\Xbf$}
  When we plug all of these known distributions into the   
  \begin{eqnarray*}
   P(\sigma^2|\ybf, \Xbf)   &\propto& \frac{P(\ybf | \betabf, \sigma^2, \Xbf)P(\betabf,\sigma^2|\Xbf)}{P(\betabf|\sigma^2, \ybf, \Xbf)} \\
     &\propto&  \frac{\sigma^{-n} \exp(-\frac{1}{2}(\ybf-\Xbf\betabf)^T\frac{1}{\sigma^2}\eye(\ybf-\Xbf\betabf))\sigma^{-2}}{\sigma^{-p} \exp(-\frac{1}{2}(\betabf-\mubf_{\betabf})^T\Sigmabf_{\betabf}^{-1}(\betabf-\mubf_{\betabf}))}
  \end{eqnarray*}
 
 which simplifies to 
    \begin{eqnarray*}
    %\lefteqn{P(\sigma^2|\ybf, \Xbf) } \\
      \propto \sigma^{-n+p-2}  \exp(-\frac{1}{2}(&&(\ybf-\Xbf\betabf)^T\frac{1}{\sigma^2}\eye(\ybf-\Xbf\betabf) \\
      &-& (\betabf-\mubf_{\betabf})^T\Sigmabf_{\betabf}^{-1}(\betabf-\mubf_{\betabf}) \,)) 
    \end{eqnarray*}
    }

    \frame[t] {%
 \frametitle{Distribution of $\sigma^2$ given observations $\ybf$ and $\Xbf$}
  With significant algebraic effort one can arrive at    
    \begin{eqnarray*}
P(\sigma^2|\ybf, \Xbf) 
      &\propto& \sigma^{-n+p-2}  \exp(-\frac{1}{2\sigma^2}(\ybf-\Xbf\mubf_{\betabf})^T(\ybf-\Xbf\mubf_{\betabf}) )
    \end{eqnarray*}
    
Remembering that $\mubf_{\betabf} = \hat \betabf$ we can rewrite this in a more familiar form, namely
      \begin{eqnarray*}
P(\sigma^2|\ybf, \Xbf) 
      &\propto& \sigma^{-n+p-2}  \exp(-\frac{1}{2\sigma^2}(\ybf-\Xbf{\hat\betabf})^T(\ybf-\Xbf\hat{\betabf}) )
    \end{eqnarray*}
      where the exponent is the sum of squared errors $SSE$.

    }
  
  
    \frame[t] {%
 \frametitle{Distribution of $\sigma^2$ given observations $\ybf$ and $\Xbf$}
  
  By inspection 
        \begin{eqnarray*}
P(\sigma^2|\ybf, \Xbf) 
      &\propto& \sigma^{-n+p-2}  \exp(-\frac{1}{2\sigma^2}(\ybf-\Xbf{\hat\betabf})^T(\ybf-\Xbf\hat{\betabf}) )
    \end{eqnarray*}

follows an scaled inverse $\chi^2$ distribution


\begin{eqnarray*}
P(\theta) &\propto&  \theta^{-(\nu/2 +1)} e^{(-\nu s^2/(2\theta))}
\end{eqnarray*}

where $\theta = \sigma^2 \implies \nu = n-p$ (i.e.~the number of degrees of freedom is the number of observations $n$ minus the number of free parameters in the model $p$ and $s^2 = \frac{1}{n-p} (\ybf-\Xbf{\hat\betabf})^T(\ybf-\Xbf\hat{\betabf})$ is the standard MSE estimate of the sample variance. 


    }

    \frame[t] {%
 \frametitle{Distribution of $\sigma^2$ given observations $\ybf$ and $\Xbf$}
  
  Note that this result 
  \begin{equation}
  \sigma^2 \sim \mathrm{Inv-}\chi^2(n-p,\frac{1}{n-p} (\ybf-\Xbf{\hat\betabf})^T(\ybf-\Xbf\hat{\betabf}))\label{eqn:scaled_in_chi} \end{equation} is exactly analogous to the following result from the classical estimation approach to linear regression. \newline
  
  From Cochran's Theorem we have 
  
  \begin{equation}\frac{SSE}{\sigma^2} = \frac{(\ybf-\Xbf{\hat\betabf})^T(\ybf-\Xbf\hat{\betabf})}{\sigma^2}  \sim \chi^2(n-p)\label{eqn:chi} \end{equation} 

To get from \eqref{eqn:scaled_in_chi} to  \eqref{eqn:chi} one can use the change of distribution formula with the change of variable $\theta^* = (\ybf-\Xbf{\hat\betabf})^T(\ybf-\Xbf\hat{\betabf})/\sigma^2$.


    }

    \frame[t] {%
 \frametitle{Distribution of output(s) given new input(s)}
  
  Last but not least we will typically be interested in prediction.  
  
 \begin{eqnarray*}
\lefteqn{P(\ynew | \Xbf, \ybf, \Xnew)}\\
 &=& \int \int P(\ynew | \betabf, \sigma^2)  P( \sigma^2 | \betabf, \Xbf, \ybf) P(\betabf |  \Xbf, \ybf)  d\betabf, d\sigma^2
 \end{eqnarray*}
 
 we will first assume, as usual that $\sigma^2$ is known and proceed with evaluating 
 \begin{eqnarray*}
\lefteqn{P(\ynew | \Xbf, \ybf, \Xnew, \sigma^2)}\\
 &=&  \int P(\ynew | \betabf, \sigma^2)  P(\betabf |  \Xbf, \ybf, \sigma^2)  d\betabf
 \end{eqnarray*}
 instead.
    }
  
    \frame[t] {%
 \frametitle{Distribution of output(s) given new input(s)}
  
We know the form of each of these expressions, the likelihood is normal as is the distribution of $\beta$ given the rest
 \begin{eqnarray*}
\lefteqn{P(\ynew | \Xbf, \ybf, \Xnew, \sigma^2)}\\
 &=&  \int P(\ynew | \betabf, \sigma^2)  P(\betabf |  \Xbf, \ybf, \sigma^2)  d\betabf
 \end{eqnarray*}

In other words

 \begin{eqnarray*}
\lefteqn{P(\ynew | \Xbf, \ybf, \Xnew, \sigma^2)}\\
 &=&  \int \N(\ynew | \Xnew\hat\betabf,\sigma^2) \N(\beta|\hat\betabf, \Sigmabf_{\betabf} )d\betabf
 \end{eqnarray*}
    }
  
      \frame[t] {%
 \frametitle{Bayes Rule for Gaussians}
  
To solve this integral we will use Bayes' rule for Gaussians (taken from Bishop).\newline

If 
\begin{eqnarray*}
P(x) &=& N(x|\mu, \Lambda^{-1}) \\
P(y|x) &=& N(y | Ax + b, L^{-1})
\end{eqnarray*}

where $x,y,$ and $\mu$ are all vectors and $\Lambda$ and $L$ are (invertable) matrices of the appropriate size then

\begin{eqnarray*}
P(y) &=& N(y|A\mu+b, \L^{-1} + A\Lambda^{-1}A^T) \\
P(x|y) &=& N(x | \Sigma ( A^TL(y-b) + \Lambda\mu),\Sigma)
\end{eqnarray*}

where $\Sigma = (\Lambda + A^TLA)^{-1}$

    }
  
      \frame[t] {%
 \frametitle{Distribution of output(s) given new input(s)}
  Since this integral is just an application of Bayes rule for Gaussians we can directly write down the solution
 \begin{eqnarray*}
\lefteqn{P(\ynew | \Xbf, \ybf, \Xnew, \sigma^2)}\\
 &=&  \int \N(\ynew | \Xnew\hat\betabf,\sigma^2) \N(\beta|\hat\betabf, \Sigmabf_{\betabf} )d\betabf
 \\
 &=& N(\ynew | \Xnew\hat\betabf, \sigma^2(\eye + \Xnew\Vbf_{\betabf}\Xnew^T)
  \end{eqnarray*}
      where $\Vbf_{\betabf} = \Sigmabf_{\betabf} / \sigma^2 = (\Xbf^T\Xbf)^{-1}$

    }
    
        \frame[t] {%
 \frametitle{Distribution of output(s) given new input(s)}
  This solution
   \begin{eqnarray*}
\lefteqn{P(\ynew | \Xbf, \ybf, \Xnew, \sigma^2)}\\
 &=& N(\ynew | \Xnew\hat\betabf, \sigma^2(\eye + \Xnew\Vbf_{\betabf}\Xnew^T)
  \end{eqnarray*}
      where $\Vbf_{\betabf} = \Sigmabf_{\betabf} / \sigma^2 = (\Xbf^T\Xbf)^{-1}$ \newline

relies upon $\sigma^2$ being known.  Our final inference objective is to come up with 

 \begin{eqnarray*}
\lefteqn{P(\ynew | \Xbf, \ybf, \Xnew)}\\
 &=& \int \int P(\ynew | \betabf, \sigma^2)  P( \sigma^2 | \betabf, \Xbf, \ybf) P(\betabf |  \Xbf, \ybf)  d\betabf, d\sigma^2 \\
 &=& \int P(\ynew | \Xbf, \ybf, \Xnew, \sigma^2) P(\sigma^2 | \Xbf, \ybf, \Xnew) d \sigma^2
 \end{eqnarray*}

where we have just derived the first term and the second we known is scaled inverse chi-squared.
    }
        
        \frame[t] {%
 \frametitle{Distribution of output(s) given new input(s)}
 
 The distributional form of  
  \begin{eqnarray*}
\lefteqn{P(\ynew | \Xbf, \ybf, \Xnew)}\\
 &=& \int P(\ynew | \Xbf, \ybf, \Xnew, \sigma^2) P(\sigma^2 | \Xbf, \ybf, \Xnew) d \sigma^2
 \end{eqnarray*}

is a multivariate Student-t distribution with center $\Xnew\hat\betabf$, squared scale marix $s^2(\eye + \Xnew \Vbf_{\betabf} \Xnew^T)$ and $n-p$ degrees of freedom (left as homework). \newline

Again this is the same result as in classical regression analysis -- the predictive distribution of a new (set of) points is Student-t when $\sigma^2$ is unknown and marginalized out.  \newline


    }
  
          \frame[t] {%
 \frametitle{Take home}
 
 \begin{itemize}
\item  The Bayesian perspective brings a new analytic perspective to the classical regression setting. 
\item In classical regression we develop estimators and then determine their distribution under repeated sampling or measurement of the underlying population. 
\item In Bayesian regression we stick with the single given dataset and calculate the uncertainty in our parameter estimates arising from the fact that we have a finite dataset. 
\item Given a single choice of prior, namely a particular {\em improper prior} we see that the posterior uncertainty regarding the model parameters corresponds exactly to the classical sampling distributions for regression estimators.
\item Other priors can be utilized.
\end{itemize}


    }
  
  
\end{document}
