\documentclass{beamer}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{setspace}
\usepackage{url}
% \usepackage{beamerthemesplit} // Activate for custom appearance

\title{Linear Regression Models\\W4315}
\author{Instructor: Dr. Yang Feng}

\date{Required Text: Applied Linear Regression
Models (4th Ed.)\\
Authors: Kutner, Nachtsheim, Neter}



\DeclareMathOperator*{\Ave}{\mathbb{E}}
\DeclareMathOperator*{\Var}{Var}

\begin{document}

\frame{\titlepage}


%\frame[t] {
% \frametitle{Not Registered Yet?}
%Fill out the form at\\
%http://tinyurl.com/mqfq95
%\bigskip
%
%Additional books we will draw material from in this course:
%\begin{itemize}
%\item Pattern Recognition and Machine Learning, by Christopher M. Bishop. Springer, 2006.
%\item Bayesian Data Analysis, Second Edition, by Andrew Gelman, John B. Carlin, Hal S. Stern, and Donald B. Rubin, Chapman \& Hall/CRC Texts in Statistical Science
%\end{itemize}
%
%
%
%
%}

\frame[t] {
 \frametitle{Course Description}
Theory and practice of regression analysis, Simple and multiple
regression, including testing, estimation, and confidence
procedures, modeling, regression diagnostics and plots, polynomial
regression, colinearity and confounding, model selection, geometry
of least squares. Extensive use of the computer to analyze data.}

\frame[t] {
 \frametitle{Philosophy and Style}
\begin{itemize}
\item Easy first half.
\item Hard second half.
\item Some digressions from the required book.
\item Understanding == proof (derivation) {\em plus} implementation.
\item Practice makes perfect.
\end{itemize}
}


\frame[t] {
 \frametitle{About me}
\begin{itemize}
\item Operations Research \& Financial Engineering PhD, 2010, Princeton University
\item First time teaching a course...
\end{itemize}
My research
\begin{itemize}
\item High-dimensional Statistical Learning
\item Variable Selection
\item Nonparametric and
Semi-parametric Statistics
\item Bioinformatics
\end{itemize}
}


\frame[t] {
 \frametitle{Course Outline}
 First half of the course is single variable linear regression.
 \begin{itemize}
 \item Least squares
 \item Maximum likelihood, normal model
 \item Tests / inferences
 \item ANOVA
 \item Diagnostics
 \item Remedial Measures
 \end{itemize}

}




\frame[t] {
\frametitle{Course Outline (Continued) }
 Second half of the course is multiple linear regression and other related topics .
\begin{itemize}
\item Multiple linear Regression
 \begin{itemize}
 \item Linear algebra review
 \item Matrix approach to linear regression
 \item Multiple predictor variables
 \item Diagnostics
 \item Tests
\end{itemize}
\item Other topics (If time permits)
\begin{itemize}
  \item Principle Component Analysis
  \item Generalized Linear Models
  \item Introduction to Bayesian Inference
\end{itemize}
\end{itemize}
}


\frame[t] {
 \frametitle{Requirements}
 \begin{itemize}
 \item Calculus
 \begin{itemize}
 \item Derivatives, gradients, convexity
 \end{itemize}
 \end{itemize}
 \begin{itemize}
 \item Linear algebra
 \begin{itemize}
 \item Matrix notation, inversion, eigenvectors, eigenvalues, rank, quadratic forms
 \end{itemize}
 \end{itemize}
 \begin{itemize}
 \item Probability
 \begin{itemize}
 \item Random variables
 \item Bayes Rule
 \end{itemize}
 \end{itemize}
 \begin{itemize}
 \item Statistics
 \begin{itemize}
 \item Expectation, variance
 \item Estimation
 \item Bias/Variance
 \item Basic probability distributions
 \end{itemize}
 \end{itemize}
 \begin{itemize}
 \item Programming
 \end{itemize}

}


\frame[t] {
 \frametitle{Software}
 \textbf{R} will be used throughout the course and it is required in all homework. An \textbf{R} tutorial session will be given on Sep 22.

Reasons for \textbf{R}:
\begin{itemize}
  \item Completely free software. Can be downloaded from http://cran.r-project.org/
  \item Available on various systems, PC, MAC, Linux, $\cdots$
  \item Advanced yet easy to use.
\end{itemize}
An Introduction to \textbf{R}:
http://cran.r-project.org/doc/manuals/R-intro.pdf
}

\frame[t] {
 \frametitle{Grading}
 \begin{itemize}
 \item Bi-weekly homework $(25\%)$
 \begin{itemize}
 \item Due every other week
 \begin{itemize}
 \item no late homework accepted
 \end{itemize}
 \item Lowest score will be dropped
 \end{itemize}
  \end{itemize}
 \begin{itemize}
 \item Exams are open book and open notes.
 \item In Class Midterm exam $(30\%)$, Wednesday, Oct 20, 2010 (tentatively).
 \item In Class Final exam $(45\%)$.
 \item Curve
 \end{itemize}

}

\frame[t] {
 \frametitle{Office Hours / Website}
 \begin{itemize}
 \item \url{http://www.stat.columbia.edu/~yangfeng}
 \item Course Materials and homeworks with the due dates will be posted on the course website.
 \item Office hours : Wednesday 2-4pm subject to change
 \item Office Location : Room 1012, SSW Building (1255 Amsterdam Avenue, between 121st and 122nd street)
 \item TA : Qinghua
 \begin{itemize}
 \item TA office hours TBA
 \end{itemize}
 \end{itemize}
}

\frame[t] {
 \frametitle{Why regression?}
 \begin{itemize}
 \item Want to model a functional relationship between an ``predictor variable'' (input, independent variable, etc.)
      and a ``response variable'' (output, dependent variable, etc.)
 \begin{itemize}
 \item Examples?
 \end{itemize}
 \end{itemize}
 \begin{itemize}
 \item But real world is noisy, no $f = ma$
 \begin{itemize}
 \item Observation noise
 \item Process noise
 \end{itemize}
 \item Two distinct goals
 \begin{itemize}
 \item Tests about natural of relationship between predictor variables and response variables
 \item Prediction
 \end{itemize}
 \end{itemize}
}

\frame[t] {
 \frametitle{History}
 \begin{itemize}
 \item Sir Francis Galton, $19^{th}$ century
 \begin{itemize}
 \item Studied the relation between heights of parents and children and noted that the children ``regressed'' to the population mean
 \end{itemize}
 \end{itemize}
 \begin{itemize}
 \item ``Regression'' stuck as the term to describe statistical relations between variables
 \end{itemize}
}

\frame[t] {
 \frametitle{Example Applications}
 Trend lines, eg. Google over 6 mo.
\begin{figure}
  \includegraphics[height=60mm]{google.png}
\end{figure}
}

\frame[t] {
 \frametitle{Others}
 \begin{itemize}
 \item Epidemiology
 \begin{itemize}
 \item Relating lifespan to obesity or smoking habits etc.
 \end{itemize}
 \end{itemize}
 \begin{itemize}
 \item Science and engineering
 \begin{itemize}
 \item Relating physical inputs to physical outputs in complex systems
 \end{itemize}
 \end{itemize}
 \begin{itemize}
 \item Grander

 \begin{center}
 \begin{figure}
  \hspace{-2cm}
  \includegraphics[height=20mm]{brain.png}
\end{figure}
\end{center}
 \end{itemize}
}

\frame[t] {
 \frametitle{Aims for the course}
\begin{itemize}
\item Given something you would like to predict and some number of covariates
\begin{itemize}
\item What kind of model should you use?
\item Which variables should you include?
\item Which transformations of variables and interaction terms should you use?
\end{itemize}
\item Given a model and some data
\begin{itemize}
\item How do you fit the model to the data?
\item How do you express confidence in the values of the model parameters?
\item How do you regularize the model to avoid over-fitting and other related issues?
\end{itemize}
\end{itemize}

}
\frame[t]{
\frametitle{Questions?}
Good time to ask now.


}


\frame[t] {
 \frametitle{Linear Regression}
\begin{itemize}
\item Want to find parameters for a function of the form
$$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$$
\item Distribution of error random variable not specified
\end{itemize}
}

\frame[t] { %%% change pic %%%
 \frametitle{Quick Example - Scatter Plot}
\begin{figure}
  \includegraphics[scale=.4]{scatter_plot.pdf}
\end{figure}
}

\frame[t] {
 \frametitle{Formal Statement of Model}
$$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$$
\begin{itemize}
\item $Y_i$ value of the response variable in the $i^{th}$ trial
\item $\beta_0$ and $\beta_1$ are parameters
\item $X_i$ is a known constant, the value of the predictor variable in the $i^{th}$ trial
\item $\epsilon_i$ is a random error term with mean $\Ave(\epsilon_i)$ and variance
$\Var(\epsilon_i)=\sigma^2 $
\item $i = 1,\ldots,n$
\end{itemize}
}

\frame[t] {
 \frametitle{Properties}
\begin{itemize}
\item The response $Y_i$ is the sum of two components
\begin{itemize}
\item Constant term $\beta_0 + \beta_1 X_i$
\item Random term $\epsilon_i$
\end{itemize}
\item The expected response is
\begin{eqnarray*}
\Ave(Y_i) &=& \Ave(\beta_0 + \beta_1 X_i + \epsilon_i)\\
 &=& \beta_0 + \beta_1 X_i + \Ave(\epsilon_i)\\
&=& \beta_0 + \beta_1 X_i
\end{eqnarray*}
\end{itemize}
}

\frame[t] {
 \frametitle{Expectation Review}
\begin{itemize}
\item Definition
$$\Ave(X) = \Ave(X) = \int X P(X) dX, \, X \in \mathcal{R}$$
\item Linearity property \begin{eqnarray*}
\Ave(aX) &=& a \Ave(X)\\
\Ave(aX + bY) &=& a\Ave(X) + b\Ave(Y)
\end{eqnarray*}

\item Obvious from definition
\end{itemize}
}

\frame[t] {
 \frametitle{Example Expectation Derivation}
$$P(X) = 2X, 0 \leq X \leq 1$$
Expectation
\begin{eqnarray*}
\Ave(X) &=& \int_0^1 X P(X) dX\\
 &=& \int_0^1 2X^2 dX\\
&=& \frac{2X^3}{3} |_0^1\\
&=& \frac{2}{3}
\end{eqnarray*}

}

\frame[t] {
 \frametitle{Expectation of a Product of Random Variables}
If X,Y are random variables with joint distribution $P(X,Y)$ then
the expectation of the product is given by
$$\Ave(XY)=\int_{XY}XYP(X,Y)dXdY.$$
}

\frame[t] {
 \frametitle{Expectation of a product of random variables}
What if X and Y are independent? If X and Y are independent with
density functions f and g respectively then
\begin{eqnarray*}
\Ave(XY)&=& \int_{XY}XYf(X)g(Y)dXdY\\
&&= \int_X\int_Y XYf(X)g(Y)dXdY\\
&&= \int_X Xf(X)[\int_Y Yg(Y)dY]dX\\
&&= \int_X Xf(X)\Ave(Y)dX\\
&&=\Ave(X)\Ave(Y)
\end{eqnarray*}
}

\frame[t] {
 \frametitle{Regression Function}
\begin{itemize}
\item The response $Y_i$ comes from a probability distribution with mean
\begin{eqnarray*}
\Ave(Y_i) &=& \beta_0 + \beta_1 X_i
\end{eqnarray*}
\item This means the regression function is
\begin{eqnarray*}
\Ave(Y) &=& \beta_0 + \beta_1 X
\end{eqnarray*}
Since the regression function relates the means of the probability
distributions of Y for a given X to the level of X

\end{itemize}
}

\frame[t] {
 \frametitle{Error Terms}
\begin{itemize}
\item The response $Y_i$ in the $i^{th}$ trial exceeds or falls short of the value of the regression function by the error term
amount $\epsilon_i$
\item The error terms $\epsilon_i$ are assumed to have constant variance $\sigma^2$

\end{itemize}
}

\frame[t] {
 \frametitle{Response Variance}
Responses $Y_i$ have the same constant variance
\begin{eqnarray*}
\Var(Y_i) &=& \Var(\beta_0 + \beta_1 X_i + \epsilon_i)\\
 &=& \Var(\epsilon_i)\\
&=& \sigma^2
\end{eqnarray*}

}

\frame[t] {
 \frametitle{Variance ($2^{nd}$ central moment) Review}
\begin{itemize}
\item Continuous distribution $$\Var(X) = \Ave(( X -  \Ave(X) ) ^ 2) = \int (X - \Ave(X))^2 P(X) dX, \, X \in \mathcal{R}$$
\item Discrete distribution $$\Var(X) = \Ave(( X -  \Ave(X) ) ^ 2) = \sum_i (X_i - \Ave(X))^2 P(X_i), \, X \in \mathcal{Z}$$
\end{itemize}
}

\frame[t] {
 \frametitle{Alternative Form for Variance}
\begin{eqnarray*}
\Var(X) &=&\Ave( ( X -  \Ave(X) ) ^ 2 ) \\
& = &\Ave( ( X ^ 2 - 2X\Ave(X) + \Ave(X) ^ 2) ) \\
&=&\Ave( X ^ 2) - 2\Ave(X)\Ave(X) + \Ave(X)  ^ 2 \\
& =&\Ave( X ^ 2) - 2\Ave(X) ^ 2 + \Ave(X) ^ 2 \\
&=&\Ave ( X ^ 2) - \Ave(X) ^ 2.
\end{eqnarray*}

}

\frame[t] {
 \frametitle{Example Variance Derivation}
$$P(X) = 2X, 0 \leq X \leq 1$$
\begin{eqnarray*}
\Var(X) &=& \Ave((X-\Ave(X))^2) = \Ave(X^2) - \Ave(X)^2 \\
 &=& \int_0^1 2X X^2 dX - (\frac{2}{3})^2\\
&=& \frac{2X^4}{4}|_0^1- \frac{4}{9} \\
&=& \frac{1}{2} -  \frac{4}{9} =  \frac{1}{18}
\end{eqnarray*}

}

\frame[t] {
 \frametitle{Variance Properties}
\begin{eqnarray*}
\Var(aX) &=& a^2 \Var(X)\\
\Var(aX + bY) &=& a^2\Var(X) + b^2\Var(Y)\, \mathrm{if} X \perp Y \\
\Var(a +cX)  &=& c^2 \Var(X)\; \mathrm{if} a, c\; \mathrm{both\ constant}
\end{eqnarray*}
More generally
\begin{eqnarray*}
\Var(\sum a_i X_i) &=& \sum_i\sum_j a_i a_j \mathrm{Cov}(X_i,X_j)
\end{eqnarray*}

}

\frame[t] {
 \frametitle{Covariance}
\begin{itemize}
\item The covariance between two real-valued random variables X and Y, with expected values $\Ave(X) = \mu$ and $\Ave(Y) = \nu$ is defined as
$$Cov(X,Y)=\Ave((X-\mu)(Y-\nu))$$
\item Which can be rewritten as\begin{eqnarray*}
Cov(X,Y)&=& \Ave(XY-\nu X-\mu Y +\mu\nu),\\
Cov(X,Y)&=& \Ave(XY)-\nu \Ave(X)-\mu \Ave(Y) +\mu\nu,\\
Cov(X,Y)&=& \Ave(XY)-\mu\nu.
\end{eqnarray*}
\end{itemize}
}

\frame[t] {
 \frametitle{Covariance of Independent Variables}
If X and Y are independent, then their covariance is zero. This
follows because under independence
$$\Ave(XY)=\Ave(X)\Ave(Y)=\mu\nu.$$
and then
$$Cov(XY)=\mu\nu-\mu\nu=0.$$
}

\end{document}
